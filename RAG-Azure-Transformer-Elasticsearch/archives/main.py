"""
RAG System - Question/R√©ponse avec Azure OpenAI et Elasticsearch

Workflow:
1. Question pos√©e par l'utilisateur
2. G√©n√©ration de l'embedding de la question
3. Recherche des documents les plus proches dans Elasticsearch
4. Construction du prompt avec les documents
5. Envoi au mod√®le Azure OpenAI
6. Retour de la r√©ponse avec sources
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import os
from dotenv import load_dotenv
import urllib3
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
from openai import AzureOpenAI

# D√©sactiver les warnings SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Charger les variables d'environnement depuis .env
load_dotenv()

# Cr√©er l'application FastAPI
app = FastAPI(title="RAG System - Azure OpenAI")

# Configuration CORS pour permettre les appels depuis un frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# CONFIGURATION - Chargement des variables d'environnement
# ============================================================================

# Elasticsearch
ES_URL = os.getenv("ES_URL", "https://elastic:Z25ft0VLU2fpiqOXRSEc@localhost:9200")
ES_INDEX = os.getenv("ES_INDEX", "docs_rag")

# Azure OpenAI
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview")

# ============================================================================
# INITIALISATION - Au d√©marrage de l'application
# ============================================================================

print(" D√©marrage du syst√®me RAG...")

# 1. Charger le mod√®le d'embeddings (pour transformer la question en vecteur)
print(" Chargement du mod√®le d'embeddings...")
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
print(" Mod√®le d'embeddings charg√© (384 dimensions)")

# 2. Connexion √† Elasticsearch
print(" Connexion √† Elasticsearch...")
es_client = Elasticsearch(
    ES_URL,
    verify_certs=False,
    ssl_show_warn=False
)
if es_client.ping():
    print(f"Connect√© √† Elasticsearch - Index: {ES_INDEX}")
else:
    print("Impossible de se connecter √† Elasticsearch")

# 3. Initialiser le client Azure OpenAI
azure_client = None
if AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:
    try:
        azure_client = AzureOpenAI(
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            api_key=AZURE_OPENAI_API_KEY,
            api_version=AZURE_OPENAI_API_VERSION
        )
        print(f"Azure OpenAI initialis√© - D√©ploiement: {AZURE_OPENAI_DEPLOYMENT}")
    except Exception as e:
        print(f"Erreur Azure OpenAI: {e}")
else:
    print(" Azure OpenAI non configur√© (v√©rifiez .env)")

print("Syst√®me RAG pr√™t!\n")

# ============================================================================
# MOD√àLES DE DONN√âES (pour l'API)
# ============================================================================

class QueryRequest(BaseModel):
    """Requ√™te de l'utilisateur"""
    question: str
    top_k: Optional[int] = 3  # Nombre de documents √† r√©cup√©rer
    include_sources: Optional[bool] = True  # Inclure les sources dans la r√©ponse
    temperature: Optional[float] = 0.7  # Cr√©ativit√© du mod√®le (0-1)
    max_tokens: Optional[int] = 1000  # Longueur max de la r√©ponse


class Source(BaseModel):
    """Un document source"""
    content: str  # Contenu du document
    metadata: dict  # M√©tadonn√©es (fichier, page, etc.)
    score: float  # Score de similarit√©


class QueryResponse(BaseModel):
    """R√©ponse du syst√®me"""
    answer: str  # R√©ponse g√©n√©r√©e
    sources: Optional[List[Source]] = None  # Documents utilis√©s
    model_used: str  # Mod√®le utilis√©


# ============================================================================
# FONCTION 1 : RECHERCHE DES DOCUMENTS DANS ELASTICSEARCH
# ============================================================================

def search_similar_chunks(question: str, top_k: int = 3):
    """
    Recherche les documents les plus similaires √† la question dans Elasticsearch
    
    √âtapes:
    1. Transforme la question en vecteur (embedding)
    2. Recherche vectorielle dans Elasticsearch (KNN)
    3. Retourne les top_k documents les plus proches
    
    Args:
        question: La question de l'utilisateur
        top_k: Nombre de documents √† retourner (d√©faut: 3)
    
    Returns:
        Liste de dictionnaires contenant content, metadata, score
    """
    
    print(f" Recherche pour: '{question}'")
    
    # √âTAPE 1: G√©n√©rer l'embedding de la question
    # Transforme le texte en vecteur de 384 dimensions
    question_embedding = embedding_model.encode([question])[0].tolist()
    print(f"   ‚Üí Embedding g√©n√©r√©: {len(question_embedding)} dimensions")
    
    # √âTAPE 2: Cr√©er la requ√™te de recherche vectorielle
    search_query = {
        "knn": {  # K-Nearest Neighbors (recherche des plus proches voisins)
            "field": "embedding",  # Champ contenant les vecteurs
            "query_vector": question_embedding,  # Vecteur de la question
            "k": top_k,  # Nombre de r√©sultats
            "num_candidates": 100  # Nombre de candidats √† √©valuer
        },
        "_source": ["content", "metadata"]  # Champs √† retourner
    }
    
    # √âTAPE 3: Ex√©cuter la recherche dans Elasticsearch
    results = es_client.search(index=ES_INDEX, body=search_query)
    
    # √âTAPE 4: Extraire les documents trouv√©s
    chunks = []
    for hit in results['hits']['hits']:
        chunks.append({
            "content": hit['_source']['content'],
            "metadata": hit['_source']['metadata'],
            "score": hit['_score']  # Score de similarit√©
        })
    
    print(f"   ‚Üí {len(chunks)} documents trouv√©s")
    
    return chunks


# ============================================================================
# FONCTION 2 : G√âN√âRATION DE LA R√âPONSE AVEC AZURE OPENAI
# ============================================================================

def generate_answer_azure(question: str, chunks: list, temperature: float = 0.7, max_tokens: int = 1000):
    """
    G√©n√®re une r√©ponse avec Azure OpenAI en utilisant les documents trouv√©s
    
    √âtapes:
    1. Construit le contexte avec les documents trouv√©s
    2. Cr√©e le prompt avec instructions + contexte + question
    3. Envoie au mod√®le Azure OpenAI
    4. Retourne la r√©ponse g√©n√©r√©e
    
    Args:
        question: La question de l'utilisateur
        chunks: Liste des documents trouv√©s
        temperature: Cr√©ativit√© (0=factuel, 1=cr√©atif)
        max_tokens: Longueur max de la r√©ponse
    
    Returns:
        R√©ponse g√©n√©r√©e par le mod√®le
    """
    
    # V√©rifier que Azure OpenAI est configur√©
    if not azure_client:
        raise HTTPException(
            status_code=500, 
            detail="Azure OpenAI non configur√©. V√©rifiez votre fichier .env"
        )
    
    print(f"ü§ñ G√©n√©ration de la r√©ponse avec {AZURE_OPENAI_DEPLOYMENT}...")
    
    # √âTAPE 1: Construire le contexte avec les documents
    # Format: [Document 1]\nContenu...\n\n[Document 2]\nContenu...
    context = "\n\n".join([
        f"[Document {i+1}]\n{chunk['content']}"
        for i, chunk in enumerate(chunks)
    ])
    
    print(f"   ‚Üí Contexte: {len(context)} caract√®res")
    
    # √âTAPE 2: Cr√©er le prompt complet
    # Le prompt contient: instructions + contexte + question
    prompt = f"""Documents de r√©f√©rence:
{context}

Question: {question}

Instructions:
- R√©ponds UNIQUEMENT en te basant sur les documents fournis ci-dessus
- Si la r√©ponse n'est pas dans les documents, dis-le clairement
- Cite les sources (Document 1, Document 2, etc.) quand c'est pertinent
- Sois concis et pr√©cis

R√©ponse:"""
    
    try:
        # √âTAPE 3: Envoyer au mod√®le Azure OpenAI
        response = azure_client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {
                    "role": "system",
                    "content": "Tu es un assistant intelligent qui r√©pond aux questions en te basant uniquement sur les documents fournis."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # √âTAPE 4: Extraire la r√©ponse
        answer = response.choices[0].message.content
        print(f"   ‚Üí R√©ponse g√©n√©r√©e: {len(answer)} caract√®res")
        
        return answer
    
    except Exception as e:
        print(f"Erreur: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur Azure OpenAI: {str(e)}")


# ============================================================================
# ENDPOINTS DE L'API
# ============================================================================

@app.get("/")
async def root():
    """Page d'accueil de l'API"""
    return {
        "message": "RAG System - Elasticsearch, Azure OpenAI",
        "version": "1.0.0",
        "status": {
            "elasticsearch": "connected" if es_client.ping() else "disconnected",
            "azure_openai": "configured" if azure_client else "not_configured"
        }
    }


@app.get("/health")
async def health():
    """V√©rification de la sant√© du syst√®me"""
    
    status = {
        "api": "ok",
        "elasticsearch": "ok" if es_client.ping() else "error",
        "azure_openai": "ok" if azure_client else "not_configured"
    }
    
    # Compter les documents index√©s
    if status["elasticsearch"] == "ok":
        try:
            count = es_client.count(index=ES_INDEX)
            status["documents_indexed"] = count['count']
        except:
            status["documents_indexed"] = 0
    
    return status


@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """
    ENDPOINT PRINCIPAL - R√©pond √† une question
    
    Workflow complet:
    1. Re√ßoit la question
    2. Recherche documents similaires dans Elasticsearch
    3. Construit le prompt avec les documents
    4. G√©n√®re la r√©ponse avec Azure OpenAI
    5. Retourne r√©ponse + sources
    """
    
    print("\n" + "="*60)
    print(f"Nouvelle question: {request.question}")
    print("="*60)
    
    try:
        # √âTAPE 1: Rechercher les documents pertinents
        chunks = search_similar_chunks(request.question, request.top_k)
        
        if not chunks:
            return QueryResponse(
                answer="Aucun document pertinent trouv√© dans la base de donn√©es.",
                sources=[],
                model_used=AZURE_OPENAI_DEPLOYMENT
            )
        
        # √âTAPE 2: G√©n√©rer la r√©ponse avec Azure OpenAI
        answer = generate_answer_azure(
            request.question, 
            chunks,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        # √âTAPE 3: Pr√©parer les sources si demand√©
        sources = None
        if request.include_sources:
            sources = [
                Source(
                    content=chunk['content'],
                    metadata=chunk['metadata'],
                    score=chunk['score']
                )
                for chunk in chunks
            ]
        
        print("‚úÖ R√©ponse pr√™te!")
        print("="*60 + "\n")
        
        # √âTAPE 4: Retourner la r√©ponse compl√®te
        return QueryResponse(
            answer=answer, 
            sources=sources,
            model_used=AZURE_OPENAI_DEPLOYMENT
        )
    
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/stats")
async def get_stats():
    """Statistiques sur les documents index√©s"""
    
    try:
        count = es_client.count(index=ES_INDEX)
        
        # R√©cup√©rer la liste des fichiers sources
        search_result = es_client.search(
            index=ES_INDEX,
            body={
                "size": 0,
                "aggs": {
                    "sources": {
                        "terms": {
                            "field": "metadata.source",
                            "size": 100
                        }
                    }
                }
            }
        )
        
        sources = []
        if 'aggregations' in search_result:
            for bucket in search_result['aggregations']['sources']['buckets']:
                sources.append({
                    "filename": bucket['key'],
                    "chunks": bucket['doc_count']
                })
        
        return {
            "total_chunks": count['count'],
            "index": ES_INDEX,
            "sources": sources,
            "model": AZURE_OPENAI_DEPLOYMENT
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# LANCEMENT DE L'APPLICATION
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    
    print("\nüåê D√©marrage du serveur FastAPI...")
    print("üìç URL: http://localhost:8000")
    print("üìö Documentation: http://localhost:8000/docs")
    print("\nAppuyez sur Ctrl+C pour arr√™ter\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)